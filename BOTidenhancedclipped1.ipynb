{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RdV6s-DZOH8R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68113f35-f32d-47b5-ce0e-672ecb04a837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Sports Video Processor...\n",
            "Using device: cpu\n",
            "\n",
            "Processing video: /content/data/video.mp4\n",
            "Output will be saved to: /content/output_tracked.mp4\n",
            "Processed 0/449 frames | FPS: 0.9 | ETA: 7.9 min | Tracks: 20\n",
            "Processed 50/449 frames | FPS: 1.6 | ETA: 4.1 min | Tracks: 25\n",
            "Processed 100/449 frames | FPS: 1.3 | ETA: 4.5 min | Tracks: 24\n",
            "Processed 150/449 frames | FPS: 1.2 | ETA: 4.2 min | Tracks: 25\n",
            "Processed 200/449 frames | FPS: 1.1 | ETA: 3.6 min | Tracks: 22\n",
            "Processed 250/449 frames | FPS: 1.1 | ETA: 3.0 min | Tracks: 22\n",
            "Processed 300/449 frames | FPS: 1.1 | ETA: 2.2 min | Tracks: 18\n",
            "Processed 350/449 frames | FPS: 1.1 | ETA: 1.5 min | Tracks: 22\n",
            "Processed 400/449 frames | FPS: 1.1 | ETA: 0.7 min | Tracks: 21\n",
            "\n",
            "==================================================\n",
            "Processing Summary\n",
            "==================================================\n",
            "Total frames processed: 449\n",
            "Total time: 397.4 seconds\n",
            "Average FPS: 1.1\n",
            "Max tracks at once: 25\n",
            "Total tracks created: 120\n",
            "Teams detected: 1\n",
            "==================================================\n",
            "\n",
            "✅ Processing completed successfully!\n",
            "Output saved to: /content/output_tracked.mp4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import deque, defaultdict\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class EnhancedSportsReIDNet(nn.Module):\n",
        "    \"\"\"Enhanced Sports-optimized ReID network with attention and multi-scale features\"\"\"\n",
        "    def __init__(self, feature_dim=768, num_classes=1000):\n",
        "        super(EnhancedSportsReIDNet, self).__init__()\n",
        "\n",
        "        # Enhanced backbone with residual connections\n",
        "        self.backbone = self._build_backbone()\n",
        "\n",
        "        # Multi-scale feature extraction with attention\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.part_pool = nn.AdaptiveMaxPool2d((6, 1))  # 6 horizontal strips for body parts\n",
        "\n",
        "        # Channel attention for feature refinement\n",
        "        self.channel_attention = ChannelAttention(512)\n",
        "\n",
        "        # Feature dimensions for different scales\n",
        "        self.global_dim = 512\n",
        "        self.part_dim = 512 * 6\n",
        "        self.color_dim = 64\n",
        "\n",
        "        # Feature projectors\n",
        "        self.global_projector = nn.Sequential(\n",
        "            nn.Linear(self.global_dim, feature_dim // 3),\n",
        "            nn.BatchNorm1d(feature_dim // 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        self.part_projector = nn.Sequential(\n",
        "            nn.Linear(self.part_dim, feature_dim // 3),\n",
        "            nn.BatchNorm1d(feature_dim // 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        self.color_projector = nn.Sequential(\n",
        "            nn.Linear(self.color_dim, feature_dim // 3),\n",
        "            nn.BatchNorm1d(feature_dim // 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # Final feature fusion with residual connection\n",
        "        self.fusion_layer = nn.Sequential(\n",
        "            nn.Linear(feature_dim, feature_dim),\n",
        "            nn.BatchNorm1d(feature_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(feature_dim, feature_dim)\n",
        "        )\n",
        "\n",
        "        # Classification head (for training if needed)\n",
        "        self.classifier = nn.Linear(feature_dim, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _build_backbone(self):\n",
        "        \"\"\"Build enhanced CNN backbone\"\"\"\n",
        "        layers = []\n",
        "\n",
        "        # First block\n",
        "        layers.extend([\n",
        "            nn.Conv2d(3, 64, 7, 2, 3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(3, 2, 1)\n",
        "        ])\n",
        "\n",
        "        # Residual blocks with increasing channels\n",
        "        in_channels = 64\n",
        "        for i, (out_channels, num_blocks) in enumerate([(64, 2), (128, 2), (256, 2), (512, 2)]):\n",
        "            for j in range(num_blocks):\n",
        "                layers.append(ResidualBlock(\n",
        "                    in_channels if j == 0 else out_channels,\n",
        "                    out_channels,\n",
        "                    stride=2 if (i > 0 and j == 0) else 1\n",
        "                ))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize network weights\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, return_features=True):\n",
        "        \"\"\"Forward pass with multi-scale feature extraction\"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Backbone feature extraction\n",
        "        features = self.backbone(x)  # [B, 512, H, W]\n",
        "\n",
        "        # Apply channel attention\n",
        "        features = self.channel_attention(features)\n",
        "\n",
        "        # Global features\n",
        "        global_feat = self.global_pool(features).view(batch_size, -1)  # [B, 512]\n",
        "        global_feat = self.global_projector(global_feat)\n",
        "\n",
        "        # Part-based features (horizontal strips)\n",
        "        part_feat = self.part_pool(features).view(batch_size, -1)  # [B, 512*6]\n",
        "        part_feat = self.part_projector(part_feat)\n",
        "\n",
        "        # Color features from input (simple but effective)\n",
        "        color_feat = self._extract_color_features(x)  # [B, 64]\n",
        "        color_feat = self.color_projector(color_feat)\n",
        "\n",
        "        # Combine all features\n",
        "        combined_feat = torch.cat([global_feat, part_feat, color_feat], dim=1)\n",
        "\n",
        "        # Final fusion with residual connection\n",
        "        fused_feat = self.fusion_layer(combined_feat)\n",
        "        final_feat = combined_feat + fused_feat  # Residual connection\n",
        "\n",
        "        # L2 normalize for cosine similarity\n",
        "        normalized_feat = F.normalize(final_feat, p=2, dim=1)\n",
        "\n",
        "        if return_features:\n",
        "            return normalized_feat\n",
        "        else:\n",
        "            # For training with classification loss\n",
        "            logits = self.classifier(normalized_feat)\n",
        "            return normalized_feat, logits\n",
        "\n",
        "    def _extract_color_features(self, x):\n",
        "        \"\"\"Extract color features from input tensor\"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Convert to HSV-like representation\n",
        "        # Simple approximation of color statistics\n",
        "        mean_rgb = torch.mean(x, dim=[2, 3])  # [B, 3]\n",
        "        std_rgb = torch.std(x, dim=[2, 3])   # [B, 3]\n",
        "\n",
        "        # Histogram-like features (simplified)\n",
        "        # Split into regions and compute statistics\n",
        "        h, w = x.size(2), x.size(3)\n",
        "\n",
        "        # Upper body region (jersey colors)\n",
        "        upper_region = x[:, :, :h//2, :]\n",
        "        upper_mean = torch.mean(upper_region, dim=[2, 3])\n",
        "        upper_std = torch.std(upper_region, dim=[2, 3])\n",
        "\n",
        "        # Lower body region\n",
        "        lower_region = x[:, :, h//2:, :]\n",
        "        lower_mean = torch.mean(lower_region, dim=[2, 3])\n",
        "        lower_std = torch.std(lower_region, dim=[2, 3])\n",
        "\n",
        "        # Combine color statistics\n",
        "        color_features = torch.cat([\n",
        "            mean_rgb, std_rgb, upper_mean, upper_std, lower_mean, lower_std\n",
        "        ], dim=1)  # [B, 18]\n",
        "\n",
        "        # Pad to desired dimension\n",
        "        if color_features.size(1) < 64:\n",
        "            padding = torch.zeros(batch_size, 64 - color_features.size(1),\n",
        "                                device=x.device, dtype=x.dtype)\n",
        "            color_features = torch.cat([color_features, padding], dim=1)\n",
        "\n",
        "        return color_features[:, :64]\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block for better gradient flow\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    \"\"\"Channel attention module for feature refinement\"\"\"\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return x * self.sigmoid(out)\n",
        "\n",
        "class AdvancedFeatureExtractor:\n",
        "    \"\"\"Advanced feature extraction with quality assessment\"\"\"\n",
        "    def __init__(self, device='cuda'):\n",
        "        self.device = device\n",
        "\n",
        "        # Enhanced preprocessing pipeline\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((256, 128)),  # Higher resolution for better features\n",
        "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # Quality assessment thresholds\n",
        "        self.min_size = (20, 40)\n",
        "        self.max_blur_variance = 50\n",
        "        self.min_brightness = 30\n",
        "        self.max_brightness = 230\n",
        "\n",
        "    def extract_roi_with_quality(self, bbox, frame):\n",
        "        \"\"\"Extract ROI with quality assessment\"\"\"\n",
        "        try:\n",
        "            x1, y1, x2, y2 = map(int, bbox)\n",
        "            h, w = frame.shape[:2]\n",
        "\n",
        "            # Clamp coordinates with margin\n",
        "            margin = 5\n",
        "            x1 = max(margin, min(x1, w - margin))\n",
        "            y1 = max(margin, min(y1, h - margin))\n",
        "            x2 = max(x1 + self.min_size[0], min(x2, w - margin))\n",
        "            y2 = max(y1 + self.min_size[1], min(y2, h - margin))\n",
        "\n",
        "            # Check minimum size\n",
        "            if (x2 - x1) < self.min_size[0] or (y2 - y1) < self.min_size[1]:\n",
        "                return None, 0.0\n",
        "\n",
        "            # Extract ROI\n",
        "            roi = frame[y1:y2, x1:x2]\n",
        "\n",
        "            # Quality assessment\n",
        "            quality_score = self._assess_roi_quality(roi)\n",
        "\n",
        "            if quality_score < 0.3:  # Minimum quality threshold\n",
        "                return None, quality_score\n",
        "\n",
        "            # Convert to RGB and apply transforms\n",
        "            roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
        "            tensor_roi = self.transform(roi_rgb)\n",
        "\n",
        "            return tensor_roi, quality_score\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, 0.0\n",
        "\n",
        "    def _assess_roi_quality(self, roi):\n",
        "        \"\"\"Assess ROI quality for better feature extraction\"\"\"\n",
        "        try:\n",
        "            # Size score\n",
        "            h, w = roi.shape[:2]\n",
        "            area = h * w\n",
        "            size_score = min(1.0, area / 10000.0)  # Normalize by expected area\n",
        "\n",
        "            # Blur detection using Laplacian variance\n",
        "            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
        "            blur_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "            blur_score = min(1.0, blur_var / 200.0)\n",
        "\n",
        "            # Brightness assessment\n",
        "            brightness = np.mean(gray)\n",
        "            if brightness < self.min_brightness or brightness > self.max_brightness:\n",
        "                brightness_score = 0.5\n",
        "            else:\n",
        "                brightness_score = 1.0\n",
        "\n",
        "            # Contrast assessment\n",
        "            contrast = np.std(gray)\n",
        "            contrast_score = min(1.0, contrast / 50.0)\n",
        "\n",
        "            # Aspect ratio score (prefer human-like ratios)\n",
        "            aspect_ratio = h / max(w, 1)\n",
        "            if 1.5 <= aspect_ratio <= 3.5:\n",
        "                aspect_score = 1.0\n",
        "            else:\n",
        "                aspect_score = 0.7\n",
        "\n",
        "            # Combined quality score\n",
        "            quality = (size_score * 0.2 +\n",
        "                      blur_score * 0.3 +\n",
        "                      brightness_score * 0.2 +\n",
        "                      contrast_score * 0.2 +\n",
        "                      aspect_score * 0.1)\n",
        "\n",
        "            return quality\n",
        "\n",
        "        except:\n",
        "            return 0.1\n",
        "\n",
        "    def extract_color_histogram(self, bbox, frame, bins=32):\n",
        "        \"\"\"Extract enhanced color histogram features\"\"\"\n",
        "        try:\n",
        "            x1, y1, x2, y2 = map(int, bbox)\n",
        "            h, w = frame.shape[:2]\n",
        "\n",
        "            # Clamp coordinates\n",
        "            x1 = max(0, min(x1, w-1))\n",
        "            y1 = max(0, min(y1, h-1))\n",
        "            x2 = max(x1+1, min(x2, w))\n",
        "            y2 = max(y1+1, min(y2, h))\n",
        "\n",
        "            roi = frame[y1:y2, x1:x2]\n",
        "\n",
        "            # Focus on upper body for jersey colors (top 60%)\n",
        "            upper_h = int(roi.shape[0] * 0.6)\n",
        "            upper_roi = roi[:upper_h, :]\n",
        "\n",
        "            # Convert to multiple color spaces\n",
        "            hsv_roi = cv2.cvtColor(upper_roi, cv2.COLOR_BGR2HSV)\n",
        "            lab_roi = cv2.cvtColor(upper_roi, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "            # HSV histogram (better for clothing colors)\n",
        "            hist_h = cv2.calcHist([hsv_roi], [0], None, [bins//2], [0, 180])\n",
        "            hist_s = cv2.calcHist([hsv_roi], [1], None, [bins//4], [0, 256])\n",
        "            hist_v = cv2.calcHist([hsv_roi], [2], None, [bins//4], [0, 256])\n",
        "\n",
        "            # LAB histogram (perceptually uniform)\n",
        "            hist_l = cv2.calcHist([lab_roi], [0], None, [bins//4], [0, 256])\n",
        "            hist_a = cv2.calcHist([lab_roi], [1], None, [bins//4], [0, 256])\n",
        "            hist_b = cv2.calcHist([lab_roi], [2], None, [bins//4], [0, 256])\n",
        "\n",
        "            # Combine histograms\n",
        "            combined_hist = np.concatenate([\n",
        "                hist_h.flatten(),\n",
        "                hist_s.flatten(),\n",
        "                hist_v.flatten(),\n",
        "                hist_l.flatten(),\n",
        "                hist_a.flatten(),\n",
        "                hist_b.flatten()\n",
        "            ])\n",
        "\n",
        "            # Normalize\n",
        "            combined_hist = combined_hist / (combined_hist.sum() + 1e-6)\n",
        "\n",
        "            return combined_hist\n",
        "\n",
        "        except Exception as e:\n",
        "            return np.zeros(bins * 6 // 4)  # Return zero vector on error\n",
        "\n",
        "class EnhancedSportsTrack:\n",
        "    \"\"\"Enhanced track class with better feature management\"\"\"\n",
        "    def __init__(self, track_id, detection, frame_idx):\n",
        "        self.track_id = track_id\n",
        "        self.bbox = detection['bbox'].copy()\n",
        "        self.confidence = detection['confidence']\n",
        "        self.class_id = detection['class_id']\n",
        "        self.class_name = detection['class_name']\n",
        "        self.birth_frame = frame_idx\n",
        "\n",
        "        # Enhanced feature management\n",
        "        self.reid_features = detection.get('reid_features', None)\n",
        "        self.color_features = detection.get('color_features', None)\n",
        "        self.quality_score = detection.get('quality_score', 0.5)\n",
        "\n",
        "        # Feature history with quality weighting\n",
        "        self.reid_history = deque(maxlen=20)  # Store more features\n",
        "        self.color_history = deque(maxlen=15)\n",
        "        self.quality_history = deque(maxlen=20)\n",
        "\n",
        "        if self.reid_features is not None:\n",
        "            self.reid_history.append(self.reid_features.copy())\n",
        "            self.quality_history.append(self.quality_score)\n",
        "\n",
        "        if self.color_features is not None:\n",
        "            self.color_history.append(self.color_features.copy())\n",
        "\n",
        "        # Enhanced state management\n",
        "        self.age = 0\n",
        "        self.hits = 1\n",
        "        self.time_since_update = 0\n",
        "        self.state = \"tentative\"\n",
        "\n",
        "        # FIX: Initialize missing occlusion attributes\n",
        "        self.was_occluded = False\n",
        "        self.occlusion_count = 0\n",
        "\n",
        "        # Adaptive thresholds based on quality\n",
        "        base_max_disappeared = 30\n",
        "        self.max_disappeared = int(base_max_disappeared * (1 + self.quality_score))\n",
        "\n",
        "        # Motion modeling\n",
        "        self.position_history = deque(maxlen=15)\n",
        "        self.velocity_history = deque(maxlen=8)\n",
        "        self.acceleration = np.array([0.0, 0.0])\n",
        "\n",
        "        center = self._get_center()\n",
        "        self.position_history.append(center)\n",
        "        self.predicted_position = center.copy()\n",
        "\n",
        "        # Enhanced confidence tracking\n",
        "        self.confidence_history = deque([detection['confidence']], maxlen=15)\n",
        "        self.avg_confidence = detection['confidence']\n",
        "        self.peak_confidence = detection['confidence']\n",
        "\n",
        "        # Track quality metrics\n",
        "        self.track_stability = 0.0\n",
        "        self.feature_consistency = 1.0\n",
        "\n",
        "        # Team assignment\n",
        "        self.team_id = None\n",
        "        self.jersey_color = None\n",
        "        self.dominant_color = None\n",
        "\n",
        "    def _get_center(self):\n",
        "        # FIX: Add bounds checking for bbox\n",
        "        try:\n",
        "            bbox = np.array(self.bbox, dtype=float)\n",
        "            return np.array([(bbox[0] + bbox[2]) / 2,\n",
        "                            (bbox[1] + bbox[3]) / 2])\n",
        "        except:\n",
        "            return np.array([0.0, 0.0])\n",
        "\n",
        "    def update(self, detection, frame_idx):\n",
        "        \"\"\"Enhanced update with quality assessment\"\"\"\n",
        "        self.age += 1\n",
        "        self.hits += 1\n",
        "        self.time_since_update = 0\n",
        "\n",
        "        # Update position and motion\n",
        "        try:\n",
        "            new_center = np.array([(detection['bbox'][0] + detection['bbox'][2]) / 2,\n",
        "                                 (detection['bbox'][1] + detection['bbox'][3]) / 2])\n",
        "            self.position_history.append(new_center)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Update bbox and confidence with type checking\n",
        "        try:\n",
        "            self.bbox = np.array(detection['bbox'], dtype=float).copy()\n",
        "            self.confidence = float(detection['confidence'])\n",
        "            self.confidence_history.append(self.confidence)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Update quality score\n",
        "        new_quality = detection.get('quality_score', 0.5)\n",
        "\n",
        "        # Update peak confidence\n",
        "        self.peak_confidence = max(self.peak_confidence, detection['confidence'])\n",
        "\n",
        "        # Update average confidence with recency weighting\n",
        "        if len(self.confidence_history) > 0:\n",
        "            weights = np.exp(np.linspace(-1, 0, len(self.confidence_history)))\n",
        "            weights /= weights.sum()\n",
        "            self.avg_confidence = np.average(list(self.confidence_history), weights=weights)\n",
        "\n",
        "        # Feature updates with quality weighting\n",
        "        if 'reid_features' in detection and detection['reid_features'] is not None:\n",
        "            self.reid_features = detection['reid_features']\n",
        "            self.reid_history.append(detection['reid_features'].copy())\n",
        "            self.quality_history.append(new_quality)\n",
        "\n",
        "            # Update feature consistency\n",
        "            if len(self.reid_history) >= 2:\n",
        "                recent_features = np.array(list(self.reid_history)[-5:])  # Last 5 features\n",
        "                similarities = []\n",
        "                for i in range(len(recent_features)-1):\n",
        "                    sim = np.dot(recent_features[i], recent_features[i+1])\n",
        "                    similarities.append(sim)\n",
        "                self.feature_consistency = np.mean(similarities) if similarities else 1.0\n",
        "\n",
        "        if 'color_features' in detection and detection['color_features'] is not None:\n",
        "            self.color_features = detection['color_features']\n",
        "            self.color_history.append(detection['color_features'].copy())\n",
        "\n",
        "        # Update track stability\n",
        "        self._update_track_stability()\n",
        "\n",
        "        # Adaptive state transitions\n",
        "        confidence_threshold = 0.4 if self.feature_consistency > 0.7 else 0.5\n",
        "        if self.hits >= 3 and self.avg_confidence > confidence_threshold:\n",
        "            self.state = \"confirmed\"\n",
        "        elif self.hits >= 8:  # Force confirmation for long tracks\n",
        "            self.state = \"confirmed\"\n",
        "\n",
        "    def _update_track_stability(self):\n",
        "        \"\"\"Update track stability metric\"\"\"\n",
        "        if len(self.position_history) < 3:\n",
        "            self.track_stability = 0.5\n",
        "            return\n",
        "\n",
        "        # Calculate position variance (lower is more stable)\n",
        "        positions = np.array(list(self.position_history))\n",
        "        if len(positions) >= 5:\n",
        "            recent_positions = positions[-5:]\n",
        "            pos_variance = np.mean(np.var(recent_positions, axis=0))\n",
        "            # Normalize variance (assuming max reasonable variance is 10000)\n",
        "            stability = max(0.0, 1.0 - pos_variance / 10000.0)\n",
        "        else:\n",
        "            stability = 0.5\n",
        "\n",
        "        # Combine with confidence stability\n",
        "        if len(self.confidence_history) >= 3:\n",
        "            conf_variance = np.var(list(self.confidence_history))\n",
        "            conf_stability = max(0.0, 1.0 - conf_variance)\n",
        "        else:\n",
        "            conf_stability = 0.5\n",
        "\n",
        "        self.track_stability = 0.7 * stability + 0.3 * conf_stability\n",
        "\n",
        "    def predict(self):\n",
        "        \"\"\"Enhanced prediction for sports movement\"\"\"\n",
        "        if self.time_since_update == 0:\n",
        "            return self.bbox.copy()\n",
        "\n",
        "        # Calculate velocity from position history\n",
        "        if len(self.position_history) >= 2:\n",
        "            current_pos = self.position_history[-1]\n",
        "            prev_pos = self.position_history[-2]\n",
        "            velocity = current_pos - prev_pos\n",
        "\n",
        "            # Add to velocity history\n",
        "            self.velocity_history.append(velocity)\n",
        "\n",
        "            # Calculate acceleration if we have velocity history\n",
        "            if len(self.velocity_history) >= 2:\n",
        "                current_vel = self.velocity_history[-1]\n",
        "                prev_vel = self.velocity_history[-2]\n",
        "                self.acceleration = 0.3 * (current_vel - prev_vel) + 0.7 * self.acceleration\n",
        "        else:\n",
        "            velocity = np.array([0.0, 0.0])\n",
        "\n",
        "        # Predict next position with motion model\n",
        "        if len(self.velocity_history) > 0:\n",
        "            avg_velocity = np.mean(list(self.velocity_history), axis=0)\n",
        "            # Scale prediction based on time since update\n",
        "            prediction_scale = min(3.0, 1.0 + 0.5 * self.time_since_update)\n",
        "            self.predicted_position = (self.position_history[-1] +\n",
        "                                     avg_velocity * prediction_scale +\n",
        "                                     0.5 * self.acceleration * prediction_scale**2)\n",
        "        else:\n",
        "            self.predicted_position = self.position_history[-1] if self.position_history else np.array([0.0, 0.0])\n",
        "\n",
        "        # Update bbox based on predicted position\n",
        "        try:\n",
        "            current_width = self.bbox[2] - self.bbox[0]\n",
        "            current_height = self.bbox[3] - self.bbox[1]\n",
        "\n",
        "            # Gradual size increase for distant predictions\n",
        "            size_scale = min(1.3, 1.0 + 0.05 * self.time_since_update)\n",
        "            pred_width = current_width * size_scale\n",
        "            pred_height = current_height * size_scale\n",
        "\n",
        "            self.bbox = np.array([\n",
        "                self.predicted_position[0] - pred_width/2,\n",
        "                self.predicted_position[1] - pred_height/2,\n",
        "                self.predicted_position[0] + pred_width/2,\n",
        "                self.predicted_position[1] + pred_height/2\n",
        "            ], dtype=float)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return self.bbox.copy()\n",
        "\n",
        "    def mark_missed(self):\n",
        "        \"\"\"Enhanced missed detection handling for sports\"\"\"\n",
        "        self.time_since_update += 1\n",
        "\n",
        "        # Check if likely occluded (based on track quality and history)\n",
        "        if (self.get_track_score() > 0.6 and self.time_since_update < 15 and\n",
        "            self.state == \"confirmed\"):\n",
        "            self.was_occluded = True\n",
        "            self.occlusion_count += 1\n",
        "\n",
        "        # Dynamic deletion thresholds based on track quality and state\n",
        "        if self.state == \"confirmed\":\n",
        "            max_disappeared = self.max_disappeared\n",
        "            if self.get_track_score() > 0.7:\n",
        "                max_disappeared = 35  # Keep high-quality tracks longer\n",
        "        else:\n",
        "            max_disappeared = 8  # Quick deletion for tentative tracks\n",
        "\n",
        "        if self.time_since_update > max_disappeared:\n",
        "            self.state = \"deleted\"\n",
        "\n",
        "    def get_weighted_reid_features(self):\n",
        "        \"\"\"Get quality-weighted average of ReID features\"\"\"\n",
        "        if not self.reid_history or not self.quality_history:\n",
        "            return self.reid_features\n",
        "\n",
        "        try:\n",
        "            features = np.array(list(self.reid_history))\n",
        "            qualities = np.array(list(self.quality_history))\n",
        "\n",
        "            # Apply recency weighting\n",
        "            recency_weights = np.exp(np.linspace(-0.5, 0, len(features)))\n",
        "\n",
        "            # Combine quality and recency weights\n",
        "            combined_weights = qualities * recency_weights\n",
        "            combined_weights = combined_weights / combined_weights.sum()\n",
        "\n",
        "            weighted_features = np.average(features, axis=0, weights=combined_weights)\n",
        "            return weighted_features\n",
        "\n",
        "        except Exception as e:\n",
        "            return self.reid_features if self.reid_features is not None else None\n",
        "\n",
        "    def get_weighted_color_features(self):\n",
        "        \"\"\"Get weighted average of color features\"\"\"\n",
        "        if not self.color_history:\n",
        "            return self.color_features\n",
        "\n",
        "        try:\n",
        "            features = np.array(list(self.color_history))\n",
        "            # Apply recency weighting\n",
        "            weights = np.exp(np.linspace(-0.3, 0, len(features)))\n",
        "            weights = weights / weights.sum()\n",
        "\n",
        "            weighted_features = np.average(features, axis=0, weights=weights)\n",
        "            return weighted_features\n",
        "\n",
        "        except:\n",
        "            return self.color_features\n",
        "\n",
        "    def get_track_score(self):\n",
        "        \"\"\"Calculate comprehensive track score\"\"\"\n",
        "        # Base score from confidence\n",
        "        conf_score = self.avg_confidence\n",
        "\n",
        "        # Stability bonus\n",
        "        stability_bonus = self.track_stability * 0.2\n",
        "\n",
        "        # Longevity bonus\n",
        "        longevity_bonus = min(0.3, self.hits / 100.0)\n",
        "\n",
        "        # Feature consistency bonus\n",
        "        consistency_bonus = self.feature_consistency * 0.1\n",
        "\n",
        "        # Quality bonus\n",
        "        quality_bonus = self.quality_score * 0.1\n",
        "\n",
        "        total_score = (conf_score + stability_bonus + longevity_bonus +\n",
        "                      consistency_bonus + quality_bonus)\n",
        "\n",
        "        return min(1.0, total_score)\n",
        "\n",
        "class SportsYOLODetector:\n",
        "    \"\"\"YOLO detector optimized for sports/soccer scenarios\"\"\"\n",
        "    def __init__(self, model_path='yolov8n.pt'):\n",
        "        self.model = YOLO(model_path)\n",
        "        self.class_names = self.model.names\n",
        "\n",
        "        # Sports-specific parameters\n",
        "        self.conf_threshold = 0.15  # Very low to catch distant players\n",
        "        self.nms_threshold = 0.5\n",
        "        self.target_classes = [0]  # Person class only\n",
        "\n",
        "        # Field region detection (to be set during processing)\n",
        "        self.field_mask = None\n",
        "        self.field_bounds = None\n",
        "\n",
        "    def set_field_region(self, frame):\n",
        "        \"\"\"Detect field region using color segmentation\"\"\"\n",
        "        try:\n",
        "            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "            # Green field detection (adjust ranges as needed)\n",
        "            lower_green = np.array([35, 40, 40])\n",
        "            upper_green = np.array([85, 255, 255])\n",
        "\n",
        "            mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "\n",
        "            # Clean up mask\n",
        "            kernel = np.ones((5,5), np.uint8)\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "            # Find largest contour (field)\n",
        "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            if contours:\n",
        "                largest_contour = max(contours, key=cv2.contourArea)\n",
        "                self.field_mask = np.zeros_like(mask)\n",
        "                cv2.fillPoly(self.field_mask, [largest_contour], 255)\n",
        "\n",
        "                # Get bounding rectangle\n",
        "                x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "                self.field_bounds = (x, y, x+w, y+h)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Field detection error: {e}\")\n",
        "            self.field_mask = None\n",
        "\n",
        "    def is_on_field(self, bbox):\n",
        "        \"\"\"Check if detection is on the field\"\"\"\n",
        "        if self.field_mask is None:\n",
        "            return True  # Accept all if no field mask\n",
        "\n",
        "        # Check center point\n",
        "        center_x = int((bbox[0] + bbox[2]) / 2)\n",
        "        center_y = int((bbox[1] + bbox[3]) / 2)\n",
        "\n",
        "        if (0 <= center_x < self.field_mask.shape[1] and\n",
        "            0 <= center_y < self.field_mask.shape[0]):\n",
        "            return self.field_mask[center_y, center_x] > 0\n",
        "\n",
        "        return False\n",
        "\n",
        "    def detect(self, frame):\n",
        "        \"\"\"Enhanced detection for sports scenarios\"\"\"\n",
        "        # Set field region on first frame\n",
        "        if self.field_mask is None:\n",
        "            self.set_field_region(frame)\n",
        "\n",
        "        # Run detection with low threshold\n",
        "        results = self.model(\n",
        "            frame,\n",
        "            conf=self.conf_threshold,\n",
        "            iou=self.nms_threshold,\n",
        "            classes=self.target_classes,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        detections = []\n",
        "\n",
        "        if results[0].boxes is not None:\n",
        "            for box in results[0].boxes:\n",
        "                bbox = box.xyxy[0].cpu().numpy()\n",
        "                confidence = float(box.conf)\n",
        "                class_id = int(box.cls)\n",
        "\n",
        "                # Sports-specific filtering\n",
        "                width = bbox[2] - bbox[0]\n",
        "                height = bbox[3] - bbox[1]\n",
        "                aspect_ratio = height / max(width, 1)\n",
        "\n",
        "                # Size filtering (remove very small detections)\n",
        "                if width < 15 or height < 30:\n",
        "                    continue\n",
        "\n",
        "                # Aspect ratio filtering for persons\n",
        "                if aspect_ratio < 1.0 or aspect_ratio > 4.0:\n",
        "                    continue\n",
        "\n",
        "                # Field region filtering\n",
        "                if not self.is_on_field(bbox):\n",
        "                    continue\n",
        "\n",
        "                # Area filtering (remove very large detections - likely false positives)\n",
        "                area = width * height\n",
        "                if area > frame.shape[0] * frame.shape[1] * 0.1:  # 10% of frame\n",
        "                    continue\n",
        "\n",
        "                detections.append({\n",
        "                    'bbox': bbox,\n",
        "                    'confidence': confidence,\n",
        "                    'class_id': class_id,\n",
        "                    'class_name': self.class_names[class_id],\n",
        "                    'area': area,\n",
        "                    'aspect_ratio': aspect_ratio\n",
        "                })\n",
        "\n",
        "        # Sort by confidence and return top detections\n",
        "        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n",
        "        return detections[:50]  # Limit to top 50 detections\n",
        "\n",
        "class EnhancedSportsTracker:\n",
        "    \"\"\"Enhanced tracker with improved ReID feature generation\"\"\"\n",
        "    def __init__(self, reid_model_path=None, max_tracks=25):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Enhanced ReID model\n",
        "        self.reid_model = EnhancedSportsReIDNet(feature_dim=768).eval().to(self.device)\n",
        "\n",
        "        if reid_model_path and os.path.exists(reid_model_path):\n",
        "            try:\n",
        "                checkpoint = torch.load(reid_model_path, map_location=self.device)\n",
        "                if 'state_dict' in checkpoint:\n",
        "                    self.reid_model.load_state_dict(checkpoint['state_dict'])\n",
        "                else:\n",
        "                    self.reid_model.load_state_dict(checkpoint)\n",
        "                print(\"Loaded custom ReID model\")\n",
        "            except Exception as e:\n",
        "                print(f\"ReID model load error: {e}\")\n",
        "\n",
        "        # Advanced feature extractor\n",
        "        self.feature_extractor = AdvancedFeatureExtractor(device=self.device)\n",
        "\n",
        "        self.tracks = []\n",
        "        self.track_id_count = 1\n",
        "        self.max_tracks = max_tracks\n",
        "\n",
        "        # Enhanced association thresholds\n",
        "        self.reid_threshold = 0.6  # Cosine similarity threshold\n",
        "        self.color_threshold = 0.7  # Color similarity threshold\n",
        "        self.iou_threshold = 0.1   # IoU threshold\n",
        "\n",
        "        # Feature matching weights\n",
        "        self.reid_weight = 0.6\n",
        "        self.color_weight = 0.3\n",
        "        self.iou_weight = 0.1\n",
        "\n",
        "        self.frame_count = 0\n",
        "\n",
        "        # Team clustering\n",
        "        self.team_clusters = {}\n",
        "        self.cluster_update_interval = 30  # Frames\n",
        "\n",
        "        # Global track management\n",
        "        self.global_track_registry = {}  # For handling re-appearances\n",
        "        self.deleted_tracks_buffer = deque(maxlen=100)\n",
        "\n",
        "        # Performance tracking\n",
        "        self.processing_times = deque(maxlen=30)\n",
        "        self.feature_extraction_times = deque(maxlen=30)\n",
        "\n",
        "    def update(self, detections, frame=None):\n",
        "        \"\"\"Enhanced tracking update with improved ReID\"\"\"\n",
        "        start_time = time.time()\n",
        "        self.frame_count += 1\n",
        "\n",
        "        try:\n",
        "            # Filter detections\n",
        "            valid_detections = self._filter_detections(detections, frame)\n",
        "\n",
        "            # Extract features for all detections\n",
        "            if frame is not None and len(valid_detections) > 0:\n",
        "                feature_start = time.time()\n",
        "                self._extract_enhanced_features(valid_detections, frame)\n",
        "                feature_time = time.time() - feature_start\n",
        "                self.feature_extraction_times.append(feature_time)\n",
        "\n",
        "            # Predict track positions\n",
        "            for track in self.tracks:\n",
        "                track.predict()\n",
        "\n",
        "            # Enhanced multi-stage association\n",
        "            matched_pairs = self._enhanced_association(valid_detections)\n",
        "\n",
        "            # Update matched tracks\n",
        "            for track_idx, det_idx in matched_pairs:\n",
        "                if track_idx < len(self.tracks) and det_idx < len(valid_detections):\n",
        "                    self.tracks[track_idx].update(valid_detections[det_idx], self.frame_count)\n",
        "\n",
        "            # Handle unmatched tracks\n",
        "            matched_track_indices = [track_idx for track_idx, _ in matched_pairs]\n",
        "            for i, track in enumerate(self.tracks):\n",
        "                if i not in matched_track_indices:\n",
        "                    track.mark_missed()\n",
        "\n",
        "            # Create new tracks\n",
        "            matched_det_indices = [det_idx for _, det_idx in matched_pairs]\n",
        "            unmatched_detections = [\n",
        "                (i, det) for i, det in enumerate(valid_detections)\n",
        "                if i not in matched_det_indices\n",
        "            ]\n",
        "\n",
        "            self._create_enhanced_tracks(unmatched_detections)\n",
        "\n",
        "            # Clean up tracks\n",
        "            self._cleanup_tracks()\n",
        "\n",
        "            # Update team clustering periodically\n",
        "            if self.frame_count % self.cluster_update_interval == 0:\n",
        "                self._update_team_clustering()\n",
        "\n",
        "            # Track performance\n",
        "            processing_time = time.time() - start_time\n",
        "            self.processing_times.append(processing_time)\n",
        "\n",
        "            return [t for t in self.tracks if t.state == \"confirmed\"]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Tracker error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _extract_enhanced_features(self, detections, frame):\n",
        "        \"\"\"Extract enhanced features for all detections\"\"\"\n",
        "        try:\n",
        "            # Batch process ReID features\n",
        "            valid_rois = []\n",
        "            valid_indices = []\n",
        "            quality_scores = []\n",
        "\n",
        "            for idx, det in enumerate(detections):\n",
        "                roi, quality = self.feature_extractor.extract_roi_with_quality(det['bbox'], frame)\n",
        "                if roi is not None:\n",
        "                    valid_rois.append(roi)\n",
        "                    valid_indices.append(idx)\n",
        "                    quality_scores.append(quality)\n",
        "\n",
        "            # Extract ReID features in batches\n",
        "            if valid_rois:\n",
        "                batch_size = 8\n",
        "                for i in range(0, len(valid_rois), batch_size):\n",
        "                    batch_rois = valid_rois[i:i+batch_size]\n",
        "                    batch_indices = valid_indices[i:i+batch_size]\n",
        "                    batch_qualities = quality_scores[i:i+batch_size]\n",
        "\n",
        "                    # Process batch\n",
        "                    tensor_batch = torch.stack(batch_rois).to(self.device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        reid_features = self.reid_model(tensor_batch)\n",
        "                        reid_features = reid_features.cpu().numpy()\n",
        "\n",
        "                    # Assign features to detections\n",
        "                    for j, det_idx in enumerate(batch_indices):\n",
        "                        detections[det_idx]['reid_features'] = reid_features[j]\n",
        "                        detections[det_idx]['quality_score'] = batch_qualities[j]\n",
        "\n",
        "            # Extract color features for all detections\n",
        "            for det in detections:\n",
        "                if 'reid_features' in det:  # Only for valid detections\n",
        "                    color_features = self.feature_extractor.extract_color_histogram(det['bbox'], frame)\n",
        "                    det['color_features'] = color_features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Enhanced feature extraction error: {e}\")\n",
        "\n",
        "    def _enhanced_association(self, detections):\n",
        "        \"\"\"Enhanced multi-stage association with improved feature matching\"\"\"\n",
        "        if not self.tracks or not detections:\n",
        "            return []\n",
        "\n",
        "        all_matches = []\n",
        "\n",
        "        # Stage 1: High-quality ReID feature matching\n",
        "        high_quality_tracks = []\n",
        "        for i, track in enumerate(self.tracks):\n",
        "            if (track.state == \"confirmed\" and\n",
        "                track.get_track_score() > 0.7 and\n",
        "                track.time_since_update <= 2 and\n",
        "                track.get_weighted_reid_features() is not None):\n",
        "                high_quality_tracks.append(i)\n",
        "\n",
        "        high_quality_detections = []\n",
        "        for i, det in enumerate(detections):\n",
        "            if (det.get('quality_score', 0) > 0.6 and\n",
        "                'reid_features' in det and det['reid_features'] is not None):\n",
        "                high_quality_detections.append(i)\n",
        "\n",
        "        if high_quality_tracks and high_quality_detections:\n",
        "            stage1_matches = self._associate_by_enhanced_features(\n",
        "                high_quality_tracks, high_quality_detections, detections\n",
        "            )\n",
        "            all_matches.extend(stage1_matches)\n",
        "\n",
        "        # Stage 2: Medium quality feature matching\n",
        "        matched_tracks = [t for t, _ in all_matches]\n",
        "        matched_dets = [d for _, d in all_matches]\n",
        "\n",
        "        medium_tracks = []\n",
        "        for i, track in enumerate(self.tracks):\n",
        "            if (i not in matched_tracks and\n",
        "                track.state == \"confirmed\" and\n",
        "                track.get_track_score() > 0.4 and\n",
        "                track.time_since_update <= 5):\n",
        "                medium_tracks.append(i)\n",
        "\n",
        "        medium_detections = []\n",
        "        for i, det in enumerate(detections):\n",
        "            if (i not in matched_dets and\n",
        "                det.get('quality_score', 0) > 0.3 and\n",
        "                'reid_features' in det):\n",
        "                medium_detections.append(i)\n",
        "\n",
        "        if medium_tracks and medium_detections:\n",
        "            stage2_matches = self._associate_by_enhanced_features(\n",
        "                medium_tracks, medium_detections, detections, threshold=0.5\n",
        "            )\n",
        "            all_matches.extend(stage2_matches)\n",
        "\n",
        "        # Stage 3: IoU-based matching for remaining\n",
        "        matched_tracks = [t for t, _ in all_matches]\n",
        "        matched_dets = [d for _, d in all_matches]\n",
        "\n",
        "        remaining_tracks = [i for i in range(len(self.tracks)) if i not in matched_tracks]\n",
        "        remaining_detections = [i for i in range(len(detections)) if i not in matched_dets]\n",
        "\n",
        "        if remaining_tracks and remaining_detections:\n",
        "            stage3_matches = self._associate_by_iou_enhanced(\n",
        "                remaining_tracks, remaining_detections, detections\n",
        "            )\n",
        "            all_matches.extend(stage3_matches)\n",
        "\n",
        "        return all_matches\n",
        "\n",
        "    def _associate_by_enhanced_features(self, track_indices, det_indices, detections, threshold=None):\n",
        "        \"\"\"Enhanced feature-based association\"\"\"\n",
        "        if not track_indices or not det_indices:\n",
        "            return []\n",
        "\n",
        "        if threshold is None:\n",
        "            threshold = self.reid_threshold\n",
        "\n",
        "        # Create comprehensive similarity matrix\n",
        "        similarity_matrix = np.zeros((len(track_indices), len(det_indices)))\n",
        "\n",
        "        for i, track_idx in enumerate(track_indices):\n",
        "            track = self.tracks[track_idx]\n",
        "            track_reid = track.get_weighted_reid_features()\n",
        "            track_color = track.get_weighted_color_features()\n",
        "\n",
        "            for j, det_idx in enumerate(det_indices):\n",
        "                det = detections[det_idx]\n",
        "                det_reid = det.get('reid_features')\n",
        "                det_color = det.get('color_features')\n",
        "\n",
        "                total_similarity = 0.0\n",
        "                weight_sum = 0.0\n",
        "\n",
        "                # ReID feature similarity\n",
        "                if track_reid is not None and det_reid is not None:\n",
        "                    try:\n",
        "                        reid_sim = np.dot(track_reid, det_reid)\n",
        "                        # Apply quality weighting\n",
        "                        quality_weight = det.get('quality_score', 0.5) * track.get_track_score()\n",
        "                        reid_sim *= quality_weight\n",
        "                        total_similarity += reid_sim * self.reid_weight\n",
        "                        weight_sum += self.reid_weight\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                # Color feature similarity\n",
        "                if track_color is not None and det_color is not None:\n",
        "                    try:\n",
        "                        # Use correlation coefficient for color histograms\n",
        "                        color_corr = np.corrcoef(track_color.flatten(), det_color.flatten())[0, 1]\n",
        "                        if not np.isnan(color_corr):\n",
        "                            color_sim = max(0, color_corr)  # Only positive correlations\n",
        "                            total_similarity += color_sim * self.color_weight\n",
        "                            weight_sum += self.color_weight\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                # IoU similarity (spatial consistency)\n",
        "                try:\n",
        "                    iou = self._calculate_iou(track.bbox, det['bbox'])\n",
        "                    total_similarity += iou * self.iou_weight\n",
        "                    weight_sum += self.iou_weight\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                # Motion consistency bonus\n",
        "                if hasattr(track, 'predicted_position'):\n",
        "                    try:\n",
        "                        det_center = np.array([(det['bbox'][0] + det['bbox'][2]) / 2,\n",
        "                                             (det['bbox'][1] + det['bbox'][3]) / 2])\n",
        "                        distance = np.linalg.norm(track.predicted_position - det_center)\n",
        "                        # Normalize distance by track size\n",
        "                        track_size = max(track.bbox[2] - track.bbox[0], track.bbox[3] - track.bbox[1])\n",
        "                        normalized_distance = distance / max(track_size, 1)\n",
        "                        motion_bonus = max(0, 0.1 * (1 - min(normalized_distance / 2.0, 1.0)))\n",
        "                        total_similarity += motion_bonus\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                # Normalize by weight sum\n",
        "                if weight_sum > 0:\n",
        "                    similarity_matrix[i, j] = total_similarity / weight_sum\n",
        "\n",
        "        # Hungarian assignment\n",
        "        try:\n",
        "            cost_matrix = 1 - similarity_matrix\n",
        "            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "            matches = []\n",
        "            for i, j in zip(row_ind, col_ind):\n",
        "                if similarity_matrix[i, j] > threshold:\n",
        "                    matches.append((track_indices[i], det_indices[j]))\n",
        "\n",
        "            return matches\n",
        "\n",
        "        except Exception as e:\n",
        "            return []\n",
        "\n",
        "    def _associate_by_iou_enhanced(self, track_indices, det_indices, detections):\n",
        "        \"\"\"Enhanced IoU-based association with motion prediction\"\"\"\n",
        "        if not track_indices or not det_indices:\n",
        "            return []\n",
        "\n",
        "        # Create enhanced IoU matrix\n",
        "        similarity_matrix = np.zeros((len(track_indices), len(det_indices)))\n",
        "\n",
        "        for i, track_idx in enumerate(track_indices):\n",
        "            track = self.tracks[track_idx]\n",
        "\n",
        "            for j, det_idx in enumerate(det_indices):\n",
        "                det = detections[det_idx]\n",
        "\n",
        "                # Standard IoU\n",
        "                iou = self._calculate_iou(track.bbox, det['bbox'])\n",
        "\n",
        "                # Size consistency bonus\n",
        "                try:\n",
        "                    track_area = (track.bbox[2] - track.bbox[0]) * (track.bbox[3] - track.bbox[1])\n",
        "                    det_area = (det['bbox'][2] - det['bbox'][0]) * (det['bbox'][3] - det['bbox'][1])\n",
        "\n",
        "                    if track_area > 0 and det_area > 0:\n",
        "                        area_ratio = min(track_area, det_area) / max(track_area, det_area)\n",
        "                        size_bonus = 0.1 * area_ratio\n",
        "                    else:\n",
        "                        size_bonus = 0\n",
        "                except:\n",
        "                    size_bonus = 0\n",
        "\n",
        "                # Confidence bonus\n",
        "                conf_bonus = 0.05 * det['confidence']\n",
        "\n",
        "                # Time penalty (tracks that haven't been updated)\n",
        "                time_penalty = 0.05 * min(track.time_since_update, 5)\n",
        "\n",
        "                total_similarity = iou + size_bonus + conf_bonus - time_penalty\n",
        "                similarity_matrix[i, j] = max(0, total_similarity)\n",
        "\n",
        "        # Hungarian assignment\n",
        "        try:\n",
        "            cost_matrix = 1 - similarity_matrix\n",
        "            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "            matches = []\n",
        "            for i, j in zip(row_ind, col_ind):\n",
        "                if similarity_matrix[i, j] > self.iou_threshold:\n",
        "                    matches.append((track_indices[i], det_indices[j]))\n",
        "\n",
        "            return matches\n",
        "\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    def _update_team_clustering(self):\n",
        "        \"\"\"Update team clustering based on color features\"\"\"\n",
        "        try:\n",
        "            # Collect color features from confirmed tracks\n",
        "            color_features = []\n",
        "            track_indices = []\n",
        "\n",
        "            for i, track in enumerate(self.tracks):\n",
        "                if (track.state == \"confirmed\" and\n",
        "                    hasattr(track, 'color_features') and\n",
        "                    track.color_features is not None):\n",
        "                    color_features.append(track.color_features)\n",
        "                    track_indices.append(i)\n",
        "\n",
        "            if len(color_features) < 5:  # Not enough samples\n",
        "                return\n",
        "\n",
        "            # Convert to numpy array\n",
        "            color_features = np.array(color_features)\n",
        "\n",
        "            # Use DBSCAN for clustering\n",
        "            dbscan = DBSCAN(eps=0.3, min_samples=3, metric='correlation')\n",
        "            clusters = dbscan.fit_predict(color_features)\n",
        "\n",
        "            # Assign team IDs based on clusters\n",
        "            for idx, cluster_id in zip(track_indices, clusters):\n",
        "                if cluster_id >= 0:  # -1 is noise in DBSCAN\n",
        "                    self.tracks[idx].team_id = cluster_id\n",
        "                    # Update jersey color based on cluster\n",
        "                    if cluster_id == 0:\n",
        "                        self.tracks[idx].jersey_color = \"Team A\"\n",
        "                    elif cluster_id == 1:\n",
        "                        self.tracks[idx].jersey_color = \"Team B\"\n",
        "                    else:\n",
        "                        self.tracks[idx].jersey_color = f\"Team {cluster_id}\"\n",
        "\n",
        "            # Store clusters for reference\n",
        "            self.team_clusters = {\n",
        "                'last_update': self.frame_count,\n",
        "                'n_clusters': len(set(clusters)) - (1 if -1 in clusters else 0)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Team clustering error: {e}\")\n",
        "\n",
        "    def _create_enhanced_tracks(self, unmatched_detections):\n",
        "        \"\"\"Create new tracks with enhanced initialization\"\"\"\n",
        "        for det_idx, det in unmatched_detections:\n",
        "            if len(self.tracks) >= self.max_tracks:\n",
        "                break\n",
        "\n",
        "            # Quality-based track creation\n",
        "            min_quality = 0.3\n",
        "            min_confidence = 0.2\n",
        "\n",
        "            if (det.get('quality_score', 0) > min_quality and\n",
        "                det['confidence'] > min_confidence):\n",
        "\n",
        "                new_track = EnhancedSportsTrack(\n",
        "                    track_id=self.track_id_count,\n",
        "                    detection=det,\n",
        "                    frame_idx=self.frame_count\n",
        "                )\n",
        "                self.tracks.append(new_track)\n",
        "                self.track_id_count += 1\n",
        "\n",
        "    def _filter_detections(self, detections, frame):\n",
        "        \"\"\"Enhanced detection filtering\"\"\"\n",
        "        if not detections:\n",
        "            return []\n",
        "\n",
        "        valid_detections = []\n",
        "        frame_area = frame.shape[0] * frame.shape[1] if frame is not None else 640 * 480\n",
        "\n",
        "        for det in detections:\n",
        "            # Basic confidence filter\n",
        "            if det['confidence'] < 0.15:\n",
        "                continue\n",
        "\n",
        "            # Enhanced bbox validation\n",
        "            if not self._is_valid_bbox_enhanced(det['bbox'], frame.shape if frame is not None else None):\n",
        "                continue\n",
        "\n",
        "            # Size filtering\n",
        "            width = det['bbox'][2] - det['bbox'][0]\n",
        "            height = det['bbox'][3] - det['bbox'][1]\n",
        "            area = width * height\n",
        "\n",
        "            # Remove very small or very large detections\n",
        "            if area < 400 or area > frame_area * 0.15:\n",
        "                continue\n",
        "\n",
        "            # Aspect ratio filtering\n",
        "            aspect_ratio = height / max(width, 1)\n",
        "            if aspect_ratio < 0.8 or aspect_ratio > 5.0:\n",
        "                continue\n",
        "\n",
        "            valid_detections.append(det)\n",
        "\n",
        "        # Sort by confidence\n",
        "        valid_detections.sort(key=lambda x: x['confidence'], reverse=True)\n",
        "\n",
        "        # Limit number of detections\n",
        "        return valid_detections[:self.max_tracks * 2]\n",
        "\n",
        "    def _is_valid_bbox_enhanced(self, bbox, frame_shape):\n",
        "        \"\"\"Enhanced bounding box validation\"\"\"\n",
        "        if frame_shape is not None:\n",
        "            h, w = frame_shape[:2]\n",
        "\n",
        "            # Check boundaries with margin\n",
        "            margin = 10\n",
        "            if (bbox[0] < -margin or bbox[1] < -margin or\n",
        "                bbox[2] > w + margin or bbox[3] > h + margin):\n",
        "                return False\n",
        "\n",
        "            # Check minimum overlap with frame\n",
        "            overlap_x1 = max(0, bbox[0])\n",
        "            overlap_y1 = max(0, bbox[1])\n",
        "            overlap_x2 = min(w, bbox[2])\n",
        "            overlap_y2 = min(h, bbox[3])\n",
        "\n",
        "            if overlap_x2 <= overlap_x1 or overlap_y2 <= overlap_y1:\n",
        "                return False\n",
        "\n",
        "            # Check minimum size\n",
        "            if (bbox[2] - bbox[0]) < 10 or (bbox[3] - bbox[1]) < 20:\n",
        "                return False\n",
        "\n",
        "        # Check bbox consistency\n",
        "        if bbox[0] >= bbox[2] or bbox[1] >= bbox[3]:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _calculate_iou(self, bbox1, bbox2):\n",
        "        \"\"\"Calculate Intersection over Union with error handling\"\"\"\n",
        "        try:\n",
        "            x1 = max(bbox1[0], bbox2[0])\n",
        "            y1 = max(bbox1[1], bbox2[1])\n",
        "            x2 = min(bbox1[2], bbox2[2])\n",
        "            y2 = min(bbox1[3], bbox2[3])\n",
        "\n",
        "            if x2 <= x1 or y2 <= y1:\n",
        "                return 0.0\n",
        "\n",
        "            intersection = (x2 - x1) * (y2 - y1)\n",
        "            area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
        "            area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
        "            union = area1 + area2 - intersection\n",
        "\n",
        "            if union <= 0:\n",
        "                return 0.0\n",
        "\n",
        "            return intersection / union\n",
        "\n",
        "        except Exception as e:\n",
        "            return 0.0\n",
        "\n",
        "    def _cleanup_tracks(self):\n",
        "        \"\"\"Clean up tracks with enhanced criteria\"\"\"\n",
        "        active_tracks = []\n",
        "\n",
        "        for track in self.tracks:\n",
        "            should_keep = True\n",
        "\n",
        "            if hasattr(track, 'state') and track.state == \"deleted\":\n",
        "                should_keep = False\n",
        "            elif hasattr(track, 'time_since_update'):\n",
        "                # Adaptive deletion based on track quality\n",
        "                track_score = track.get_track_score() if hasattr(track, 'get_track_score') else 0.5\n",
        "\n",
        "                if track.state == \"confirmed\":\n",
        "                    max_disappeared = int(30 * (1 + track_score))\n",
        "                else:\n",
        "                    max_disappeared = 8\n",
        "\n",
        "                if track.time_since_update > max_disappeared:\n",
        "                    should_keep = False\n",
        "\n",
        "            if should_keep:\n",
        "                active_tracks.append(track)\n",
        "\n",
        "        self.tracks = active_tracks\n",
        "\n",
        "    def get_performance_stats(self):\n",
        "        \"\"\"Get tracker performance statistics\"\"\"\n",
        "        stats = {}\n",
        "\n",
        "        if self.processing_times:\n",
        "            avg_fps = 1.0 / np.mean(self.processing_times)\n",
        "            stats['avg_fps'] = avg_fps\n",
        "            stats['avg_processing_time'] = np.mean(self.processing_times)\n",
        "\n",
        "        if self.feature_extraction_times:\n",
        "            stats['avg_feature_time'] = np.mean(self.feature_extraction_times)\n",
        "            stats['feature_time_ratio'] = (np.mean(self.feature_extraction_times) /\n",
        "                                          max(np.mean(self.processing_times), 0.001))\n",
        "\n",
        "        stats['active_tracks'] = len([t for t in self.tracks if t.state == \"confirmed\"])\n",
        "        stats['total_tracks'] = len(self.tracks)\n",
        "        stats['tracks_created'] = self.track_id_count - 1\n",
        "\n",
        "        return stats\n",
        "\n",
        "class SportsVideoProcessor:\n",
        "    \"\"\"Video processor optimized for sports tracking\"\"\"\n",
        "    def __init__(self, model_path=None, max_frames=1000):\n",
        "        print(\"Initializing Sports Video Processor...\")\n",
        "        self.tracker = EnhancedSportsTracker(reid_model_path=model_path)\n",
        "        self.detector = SportsYOLODetector()\n",
        "        self.max_frames = max_frames\n",
        "        self.frame_skip = 1\n",
        "        self.processed_frames = 0\n",
        "\n",
        "        # Visualization settings\n",
        "        self.colors = [(np.random.randint(50, 255),\n",
        "                       np.random.randint(50, 255),\n",
        "                       np.random.randint(50, 255)) for _ in range(100)]\n",
        "\n",
        "        # Team colors (can be customized)\n",
        "        self.team_colors = {\n",
        "            \"Team A\": (255, 0, 0),    # Red\n",
        "            \"Team B\": (0, 0, 255),    # Blue\n",
        "            \"Referee\": (0, 255, 0),   # Green\n",
        "            None: (255, 255, 255)     # White for unknown\n",
        "        }\n",
        "\n",
        "        # Performance tracking\n",
        "        self.processing_times = deque(maxlen=100)\n",
        "        self.start_time = None\n",
        "\n",
        "    def process_frame(self, frame, frame_idx):\n",
        "        \"\"\"Process a single frame\"\"\"\n",
        "        try:\n",
        "            frame_start = time.time()\n",
        "\n",
        "            # Detect objects\n",
        "            detections = self.detector.detect(frame)\n",
        "\n",
        "            # Update tracker\n",
        "            tracks = self.tracker.update(detections, frame)\n",
        "\n",
        "            # Visualize results\n",
        "            vis_frame = self._visualize(frame, tracks, detections, frame_idx)\n",
        "\n",
        "            # Performance tracking\n",
        "            processing_time = time.time() - frame_start\n",
        "            self.processing_times.append(processing_time)\n",
        "\n",
        "            return vis_frame, tracks, True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Frame {frame_idx} processing error: {e}\")\n",
        "            return frame.copy(), [], False\n",
        "\n",
        "    def _visualize(self, frame, tracks, detections, frame_idx):\n",
        "        \"\"\"Enhanced visualization for sports tracking - FIX: Added proper type checking\"\"\"\n",
        "        vis_frame = frame.copy()\n",
        "\n",
        "        # Draw field boundaries if available\n",
        "        if self.detector.field_bounds is not None:\n",
        "            try:\n",
        "                x1, y1, x2, y2 = map(int, self.detector.field_bounds)\n",
        "                cv2.rectangle(vis_frame, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Draw detections (transparent)\n",
        "        for det in detections:\n",
        "            try:\n",
        "                bbox = det['bbox']\n",
        "                if bbox is not None and len(bbox) >= 4:\n",
        "                    x1, y1, x2, y2 = map(int, bbox[:4])\n",
        "                    cv2.rectangle(vis_frame, (x1, y1), (x2, y2), (200, 200, 200), 1)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Draw tracks with team information\n",
        "        for track in tracks:\n",
        "            try:\n",
        "                # Determine color based on team\n",
        "                if hasattr(track, 'jersey_color') and track.jersey_color in self.team_colors:\n",
        "                    color = self.team_colors[track.jersey_color]\n",
        "                elif hasattr(track, 'team_id'):\n",
        "                    color = self.team_colors.get(f\"Team {track.team_id}\", (255, 255, 255))\n",
        "                else:\n",
        "                    color = self.colors[track.track_id % len(self.colors)]\n",
        "\n",
        "                # FIX: Ensure bbox is valid before drawing\n",
        "                bbox = track.bbox\n",
        "                if bbox is None or len(bbox) < 4:\n",
        "                    continue\n",
        "\n",
        "                # Convert to integers with bounds checking\n",
        "                try:\n",
        "                    bbox_int = [int(float(x)) for x in bbox[:4]]\n",
        "                    x1, y1, x2, y2 = bbox_int\n",
        "\n",
        "                    # Sanity check coordinates\n",
        "                    if x1 >= x2 or y1 >= y2:\n",
        "                        continue\n",
        "\n",
        "                except (ValueError, TypeError, OverflowError):\n",
        "                    continue\n",
        "\n",
        "                # Draw bounding box\n",
        "                thickness = 3 if track.state == \"confirmed\" else 1\n",
        "                cv2.rectangle(vis_frame, (x1, y1), (x2, y2), color, thickness)\n",
        "\n",
        "                # Draw team/jersey information if available\n",
        "                team_text = \"\"\n",
        "                if hasattr(track, 'jersey_color'):\n",
        "                    team_text = track.jersey_color\n",
        "                elif hasattr(track, 'team_id') and track.team_id is not None:\n",
        "                    team_text = f\"Team {track.team_id}\"\n",
        "\n",
        "                # Create label\n",
        "                label_parts = [f\"ID:{track.track_id}\"]\n",
        "                if team_text:\n",
        "                    label_parts.append(team_text)\n",
        "                if hasattr(track, 'time_since_update') and track.time_since_update > 0:\n",
        "                    label_parts.append(f\"({track.time_since_update})\")\n",
        "\n",
        "                label = \" \".join(label_parts)\n",
        "\n",
        "                # Draw label background\n",
        "                try:\n",
        "                    (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
        "                    cv2.rectangle(vis_frame, (x1, y1 - text_h - 5), (x1 + text_w + 5, y1), color, -1)\n",
        "\n",
        "                    # Draw label text\n",
        "                    cv2.putText(vis_frame, label, (x1 + 2, y1 - 5),\n",
        "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                # Draw trajectory\n",
        "                if hasattr(track, 'position_history') and len(track.position_history) > 1:\n",
        "                    try:\n",
        "                        points = np.array(track.position_history, dtype=np.int32)\n",
        "                        if len(points) > 1:\n",
        "                            cv2.polylines(vis_frame, [points], False, color, 1)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "            except Exception as e:\n",
        "                # Skip this track if there's any error in visualization\n",
        "                continue\n",
        "\n",
        "        # Draw status information\n",
        "        status_info = [\n",
        "            f\"Frame: {frame_idx}\",\n",
        "            f\"Detections: {len(detections)}\",\n",
        "            f\"Tracks: {len(tracks)}\",\n",
        "            f\"Teams: {self.tracker.team_clusters.get('n_clusters', 0)}\",\n",
        "            f\"FPS: {1.0/np.mean(self.processing_times):.1f}\" if self.processing_times else \"FPS: --\"\n",
        "        ]\n",
        "\n",
        "        for i, text in enumerate(status_info):\n",
        "            try:\n",
        "                cv2.putText(vis_frame, text, (10, 20 + i*20),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return vis_frame\n",
        "\n",
        "    def process_video(self, input_path, output_path):\n",
        "        \"\"\"Process video file\"\"\"\n",
        "        print(f\"\\nProcessing video: {input_path}\")\n",
        "        print(f\"Output will be saved to: {output_path}\")\n",
        "\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Error opening video: {input_path}\")\n",
        "            return False\n",
        "\n",
        "        # Get video properties\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Setup output video\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "        if not out.isOpened():\n",
        "            print(f\"Error creating output video: {output_path}\")\n",
        "            cap.release()\n",
        "            return False\n",
        "\n",
        "        self.start_time = time.time()\n",
        "        frame_idx = 0\n",
        "\n",
        "        while cap.isOpened() and frame_idx < self.max_frames:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_idx % self.frame_skip == 0:\n",
        "                # Process frame\n",
        "                vis_frame, _, success = self.process_frame(frame, frame_idx)\n",
        "\n",
        "                if success:\n",
        "                    out.write(vis_frame)\n",
        "                    self.processed_frames += 1\n",
        "\n",
        "                # Print progress\n",
        "                if frame_idx % 50 == 0:\n",
        "                    elapsed = time.time() - self.start_time\n",
        "                    fps = (frame_idx + 1) / elapsed if elapsed > 0 else 0\n",
        "                    remaining = (total_frames - frame_idx) / max(fps, 0.1) / 60\n",
        "                    print(f\"Processed {frame_idx}/{total_frames} frames | \"\n",
        "                          f\"FPS: {fps:.1f} | \"\n",
        "                          f\"ETA: {remaining:.1f} min | \"\n",
        "                          f\"Tracks: {len(self.tracker.tracks)}\")\n",
        "\n",
        "            frame_idx += 1\n",
        "\n",
        "        # Release resources\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        # Print summary\n",
        "        self._print_summary()\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _print_summary(self):\n",
        "        \"\"\"Print processing summary\"\"\"\n",
        "        if self.start_time is None:\n",
        "            return\n",
        "\n",
        "        duration = time.time() - self.start_time\n",
        "        avg_fps = self.processed_frames / duration if duration > 0 else 0\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Processing Summary\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Total frames processed: {self.processed_frames}\")\n",
        "        print(f\"Total time: {duration:.1f} seconds\")\n",
        "        print(f\"Average FPS: {avg_fps:.1f}\")\n",
        "        print(f\"Max tracks at once: {self.tracker.max_tracks}\")\n",
        "        print(f\"Total tracks created: {self.tracker.track_id_count - 1}\")\n",
        "\n",
        "        if hasattr(self.tracker, 'team_clusters'):\n",
        "            print(f\"Teams detected: {self.tracker.team_clusters.get('n_clusters', 0)}\")\n",
        "\n",
        "        print(\"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration - SET YOUR PATHS HERE\n",
        "    input_video = \"/content/data/video.mp4\"  # Your video path\n",
        "    output_video = \"/content/output_tracked.mp4\"  # Output path\n",
        "    reid_model_path = None  # Optional path to custom ReID model\n",
        "\n",
        "    # Initialize processor\n",
        "    processor = SportsVideoProcessor(\n",
        "        model_path=reid_model_path,\n",
        "        max_frames=1000  # Set to None to process entire video\n",
        "    )\n",
        "\n",
        "    # Verify input file exists\n",
        "    if not os.path.exists(input_video):\n",
        "        print(f\"\\nERROR: Input video not found at {input_video}\")\n",
        "        print(\"Please verify the path exists and try again.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Process video\n",
        "    success = processor.process_video(input_video, output_video)\n",
        "\n",
        "    if success:\n",
        "        print(f\"\\n✅ Processing completed successfully!\")\n",
        "        print(f\"Output saved to: {output_video}\")\n",
        "    else:\n",
        "        print(\"\\n❌ Processing failed\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78814a91",
        "outputId": "ad486620-2918-4634-9808-2e4fe9cefc38"
      },
      "source": [
        "%pip install ultralytics"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.179)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.15)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
          ]
        }
      ]
    }
  ]
}